@article{Schultz2021,
  author = {Schultz, Martin and Betancourt, Clara and Gong, Bing and Kleinert, Felix and Langguth, Michael and Leufen, Lukas and Mozaffari, Amirpasha and Stadtler, Scarlet},
  year = {2021},
  month = {02},
  title = {Can deep learning beat numerical weather prediction?},
  volume = {379},
  pages = {20200097},
  journal = {Philosophical Transactions of The Royal Society A Mathematical Physical and Engineering Sciences},
  doi = {10.1098/rsta.2020.0097}
}

@article{Google2023,
  author = {Remi Lam  and Alvaro Sanchez-Gonzalez  and Matthew Willson  and Peter Wirnsberger  and Meire Fortunato  and Ferran Alet  and Suman Ravuri  and Timo Ewalds  and Zach Eaton-Rosen  and Weihua Hu  and Alexander Merose  and Stephan Hoyer  and George Holland  and Oriol Vinyals  and Jacklynn Stott  and Alexander Pritzel  and Shakir Mohamed  and Peter Battaglia },
  title = {Learning skillful medium-range global weather forecasting},
  journal = {Science},
  volume = {382},
  number = {6677},
  pages = {1416-1421},
  year = {2023},
  doi = {10.1126/science.adi2336},
  URL = {https://www.science.org/doi/abs/10.1126/science.adi2336},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.adi2336},
  abstract = {Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy but does not directly use historical weather data to improve the underlying model. Here, we introduce GraphCast, a machine learning?based method trained directly from reanalysis data. It predicts hundreds of weather variables for the next 10 days at 0.25Åã resolution globally in under 1 minute. GraphCast significantly outperforms the most accurate operational deterministic systems on 90\% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclone tracking, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting and helps realize the promise of machine learning for modeling complex dynamical systems. The numerical models used to predict weather are large, complex, and computationally demanding and do not learn from past weather patterns. Lam et al. introduced a machine learning?based method that has been trained directly from reanalysis data of past atmospheric conditions. In this way, the authors were able to quickly predict hundreds of weather variables globally up to 10 days in advance and at high resolution. Their predictions were more accurate than those of traditional weather models in 90\% of tested cases and displayed better severe event prediction for tropical cyclones, atmospheric rivers, and extreme temperatures. ?H. Jesse Smith Machine learning leads to better, faster, and cheaper weather forecasting.}
}

@article{doi:10.1146/annurev.fluid.30.1.329,
author = {Chen, Shiyi and Doolen, Gary D.},
title = {LATTICE BOLTZMANN METHOD FOR FLUID FLOWS},
journal = {Annual Review of Fluid Mechanics},
volume = {30},
number = {1},
pages = {329-364},
year = {1998},
doi = {10.1146/annurev.fluid.30.1.329},
URL = {https://doi.org/10.1146/annurev.fluid.30.1.329},
eprint = {https://doi.org/10.1146/annurev.fluid.30.1.329},
abstract = { ? Abstract?We present an overview of the lattice Boltzmann method (LBM), a parallel and efficient algorithm for simulating single-phase and multiphase fluid flows and for incorporating additional physical complexities. The LBM is especially useful for modeling complicated boundary conditions and multiphase interfaces. Recent extensions of this method are described, including simulations of fluid turbulence, suspension flows, and reaction diffusion systems. }
}

@article{Hornik1989MultilayerFN,
  title={Multilayer feedforward networks are universal approximators},
  author={Kurt Hornik and Maxwell B. Stinchcombe and Halbert L. White},
  journal={Neural Networks},
  year={1989},
  volume={2},
  pages={359-366},
  url={https://api.semanticscholar.org/CorpusID:2757547}
}

@article{Inamuro1990NumericalSO,
  author = {Inamuro, Takaji and Sturtevant, Bradford},
  title = "{Numerical study of discrete-velocity gases}",
  journal = {Physics of Fluids A: Fluid Dynamics},
  volume = {2},
  number = {12},
  pages = {2196-2203},
  year = {1990},
  month = {12},
  issn = {0899-8213},
  doi = {10.1063/1.857825},
  url = {https://doi.org/10.1063/1.857825},
  eprint = {https://pubs.aip.org/aip/pof/article-pdf/2/12/2196/12512365/2196\_1\_online.pdf},
}

@article{PhysRev.94.511,
  title = {A Model for Collision Processes in Gases. I. Small Amplitude Processes in Charged and Neutral One-Component Systems},
  author = {Bhatnagar, P. L. and Gross, E. P. and Krook, M.},
  journal = {Phys. Rev.},
  volume = {94},
  issue = {3},
  pages = {511-525},
  numpages = {0},
  year = {1954},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.94.511},
  url = {https://link.aps.org/doi/10.1103/PhysRev.94.511}
}

@article {CHEN2021114451,
title = {2-D regional short-term wind speed forecast based on CNN-LSTM deep learning model},
journal = {Energy Conversion and Management},
volume = {244},
pages = {114451},
year = {2021},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2021.114451},
url = {https://www.sciencedirect.com/science/article/pii/S0196890421006270},
author = {Yaoran Chen and Yan Wang and Zhikun Dong and Jie Su and Zhaolong Han and Dai Zhou and Yongsheng Zhao and Yan Bao},
keywords = {Regional wind speed prediction, CNN, LSTM, Temporal series fitness, Spatial distribution},
abstract = {Short-term wind speed forecast is of great importance to wind farm regulation and its early warning. Previous studies mainly focused on the prediction at a single location but few extended the task to 2-D wind plane. In this study, a novel deep learning model was proposed for a 2-D regional wind speed forecast, using the combination of the auto-encoder of convolutional neural network (CNN) and the long short-term memory unit (LSTM). The 12-hidden-layer deep CNN was adopted to encode the high dimensional 2-D input into the embedding vector and inversely, to decode such latent representation after it was predicted by the LSTM module based on historical data. The model performance was compared with parallel models under different criteria, including MAE, RMSE and R2, all showing stable and considerable enhancements. For instance, the overall MAE value dropped to 0.35?m/s for the current model, which is 32.7%, 28.8% and 18.9% away from the prediction results using the persistence, basic ANN and LSTM model. Moreover, comprehensive discussions were provided from both temporal and spatial views of analysis, revealing that the current model can not only offer an accurate wind speed forecast along timeline (R2 equals to 0.981), but also give a distinct estimation of the spatial wind speed distribution in 2-D wind farm.}
}

@Article {Satofuka1999,
author={Satofuka, N.
and Nishioka, T.},
title={Parallelization of lattice Boltzmann method for incompressible flow computations},
journal={Computational Mechanics},
year={1999},
month={Mar},
day={01},
volume={23},
number={2},
pages={164-171},
abstract={Parallel computation of the two and three-dimensional decaying homogeneous isotropic turbulence using the lattice Boltzmann method are presented. BGK type approximation for collision term in 9 velocity square lattice model is used. It is found that the lattice Boltzmann method is able to reproduce the dynamics of decaying turbulence and could be an alternative for solving the Navier-Stokes equations. The lattice Boltzmann method is parallelized by using domain decomposition and implemented on a distributed memory computer, Hitachi SR2201. It is found that vertical decomposition gives the highest speedup. In the case of horizontal decomposition the longer the number of lattice units in horizontal direction of each subdomain, the shorter the CPU time. Extension to three-dimension is carried out using 15 velocity cubic lattice model. Compared with the result of two-dimensional case, a higher speedup is obtained than in the three-dimensional simulation. Further investigation is needed on the accuracy and efficiency of cubic lattice BGK model.},
issn={1432-0924},
doi={10.1007/s004660050397},
url={https://doi.org/10.1007/s004660050397}
}

@article{PINNs2020,
author = {Maziar Raissi  and Alireza Yazdani  and George Em Karniadakis },
title = {Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations},
journal = {Science},
volume = {367},
number = {6481},
pages = {1026-1030},
year = {2020},
doi = {10.1126/science.aaw4741},
URL = {https://www.science.org/doi/abs/10.1126/science.aaw4741},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aaw4741},
abstract = {Quantifying fluid flow is relevant to disciplines ranging from geophysics to medicine. Flow can be experimentally visualized using, for example, smoke or contrast agents, but extracting velocity and pressure fields from this information is tricky. Raissi et al. developed a machine-learning approach to tackle this problem. Their method exploits the knowledge of Navier-Stokes equations, which govern the dynamics of fluid flow in many scientifically relevant situations. The authors illustrate their approach using examples such as blood flow in an aneurysm. Science, this issue p. 1026 A machine learning approach exploiting the knowledge of Navier-Stokes equations can extract detailed fluid flow information. For centuries, flow visualization has been the art of making fluid motion visible in physical and biological systems. Although such flow patterns can be, in principle, described by the Navier-Stokes equations, extracting the velocity and pressure fields directly from the images is challenging. We addressed this problem by developing hidden fluid mechanics (HFM), a physics-informed deep-learning framework capable of encoding the Navier-Stokes equations into the neural networks while being agnostic to the geometry or the initial and boundary conditions. We demonstrate HFM for several physical and biomedical problems by extracting quantitative information for which direct measurements may not be possible. HFM is robust to low resolution and substantial noise in the observation data, which is important for potential applications.}
}

@Inbook{Lesieur1990,
author="Lesieur, Marcel",
title="Diffusion of Passive Scalars",
bookTitle="Turbulence in Fluids: Stochastic and Numerical Modelling",
year="1990",
publisher="Springer Netherlands",
address="Dordrecht",
pages="205-241",
abstract="We have already seen that under certain approximations consisting in neglecting the buoyancy in the Boussinesq approximation derived from the Navier-Stokes equations, the temperature {\$}{\$}T{\backslash}left( {\{}{\backslash}vec x,t{\}} {\backslash}right){\$}{\$}satisfied a passive scalar type diffusion equation(VIII-1-1){\$}{\$}{\backslash}frac{\{}{\{}{\backslash}partial T{\}}{\}}{\{}{\{}{\backslash}partial t{\}}{\}} + {\backslash}vec u.{\backslash}vec {\backslash}nabla T = {\backslash}kappa {\{}{\backslash}nabla ^2{\}}T{\$}{\$}and was simply transported by the fluid particle (and diffused by molecular effects) without any action on the flow dynamics. More generally, one can consider any passive quantity which diffuses according to eq. (VIII-1-1), such as a dye which marks the flow. The Schmidt number of the passive scalar is where k is the molecular diffusivity of the scalar. It corresponds to the Prandtl number when the passive scalar is the temperature. Since we will consider only one diffusing quantity here, we will associate it with the temperature, and speak of the Prandtl number of the passive scalar.",
isbn="978-94-009-0533-7",
doi="10.1007/978-94-009-0533-7_8",
url="https://doi.org/10.1007/978-94-009-0533-7_8"
}

@Article{app13126892,
AUTHOR = {Pateras, Joseph and Rana, Pratip and Ghosh, Preetam},
TITLE = {A Taxonomic Survey of Physics-Informed Machine Learning},
JOURNAL = {Applied Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {12},
ARTICLE-NUMBER = {6892},
URL = {https://www.mdpi.com/2076-3417/13/12/6892},
ISSN = {2076-3417},
ABSTRACT = {Physics-informed machine learning (PIML) refers to the emerging area of extracting physically relevant solutions to complex multiscale modeling problems lacking sufficient quantity and veracity of data with learning models informed by physically relevant prior information. This work discusses the recent critical advancements in the PIML domain. Novel methods and applications of domain decomposition in physics-informed neural networks (PINNs) in particular are highlighted. Additionally, we explore recent works toward utilizing neural operator learning to intuit relationships in physics systems traditionally modeled by sets of complex governing equations and solved with expensive differentiation techniques. Finally, expansive applications of traditional physics-informed machine learning and potential limitations are discussed. In addition to summarizing recent work, we propose a novel taxonomic structure to catalog physics-informed machine learning based on how the physics-information is derived and injected into the machine learning process. The taxonomy assumes the explicit objectives of facilitating interdisciplinary collaboration in methodology, thereby promoting a wider characterization of what types of physics problems are served by the physics-informed learning machines and assisting in identifying suitable targets for future work. To summarize, the major twofold goal of this work is to summarize recent advancements and introduce a taxonomic catalog for applications of physics-informed machine learning.},
DOI = {10.3390/app13126892}
}

@article{OH20041311,
title = {GPU implementation of neural networks},
journal = {Pattern Recognition},
volume = {37},
number = {6},
pages = {1311-1314},
year = {2004},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2004.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0031320304000524},
author = {Kyoung-Su Oh and Keechul Jung},
keywords = {Graphics processing unit(GPU), Neural network(NN), Multi-layer perceptron, Text detection},
abstract = {Graphics processing unit (GPU) is used for a faster artificial neural network. It is used to implement the matrix multiplication of a neural network to enhance the time performance of a text detection system. Preliminary results produced a 20-fold performance enhancement using an ATI RADEON 9700 PRO board. The parallelism of a GPU is fully utilized by accumulating a lot of input feature vectors and weight vectors, then converting the many inner-product operations into one matrix operation. Further research areas include benchmarking the performance with various hardware and GPU-aware learning algorithms.}
}

@article{doi:10.1126/science.1127647,
author = {G. E. Hinton  and R. R. Salakhutdinov },
title = {Reducing the Dimensionality of Data with Neural Networks},
journal = {Science},
volume = {313},
number = {5786},
pages = {504-507},
year = {2006},
doi = {10.1126/science.1127647},
URL = {https://www.science.org/doi/abs/10.1126/science.1127647},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1127647},
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such ÅgautoencoderÅh networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.}}

@misc{Seizonken2004,
  author = {ãûìsëÂäwê∂ë∂åóå§ãÜèä},
  title = {ê∂ë∂åóÉfÅ[É^ÉxÅ[ÉX.},
  year = {2004},
  note = {http://database.rish.kyoto-u.ac.jp/, (éQè∆2024-01-16)}
}

@misc{JMBSC2022,
  author = {àÍî ç‡ícñ@êl ãCè€ã∆ñ±éxâáÉZÉìÉ^Å[},
  title = {ÉÅÉ\êîíló\ïÒÉÇÉfÉãGPVÅiMSMÅj},
  year = {2022},
  note = {http://www.jmbsc.or.jp/jp/online/file/f-online10200.html, (éQè∆2024-01-16)}
}

@article{Kingma2014AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:6628106}
}

@misc{GoogleColaboratory,
  author = {Google Research},
  title = {Colaboratory},
  year = {2023},
  note  = {https://research.google.com/colaboratory/faq.html, (éQè∆2022-03-24)}
}

@incollection{NEURIPS2019-9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@Article{Yamashita2018,
author={Yamashita, Rikiya
and Nishio, Mizuho
and Do, Richard Kinh Gian
and Togashi, Kaori},
title={Convolutional neural networks: an overview and application in radiology},
journal={Insights into Imaging},
year={2018},
month={Aug},
day={01},
volume={9},
number={4},
pages={611-629},
abstract={Convolutional neural network (CNN), a class of artificial neural networks that has become dominant in various computer vision tasks, is attracting interest across a variety of domains, including radiology. CNN is designed to automatically and adaptively learn spatial hierarchies of features through backpropagation by using multiple building blocks, such as convolution layers, pooling layers, and fully connected layers. This review article offers a perspective on the basic concepts of CNN and its application to various radiological tasks, and discusses its challenges and future directions in the field of radiology. Two challenges in applying CNN to radiological tasks, small dataset and overfitting, will also be covered in this article, as well as techniques to minimize them. Being familiar with the concepts and advantages, as well as limitations, of CNN is essential to leverage its potential in diagnostic radiology, with the goal of augmenting the performance of radiologists and improving patient care.},
issn={1869-4101},
doi={10.1007/s13244-018-0639-9},
url={https://doi.org/10.1007/s13244-018-0639-9}
}

@article{10.1162/neco.1997.9.8.1735,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = {nov},
pages = {1735-1780},
numpages = {46}
}

@ARTICLE{485891,
  author={Jain, A.K. and Jianchang Mao and Mohiuddin, K.M.},
  journal={Computer}, 
  title={Artificial neural networks: a tutorial}, 
  year={1996},
  volume={29},
  number={3},
  pages={31-44},
  doi={10.1109/2.485891}}


@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{10.5555/3122009.3242010,
author = {Baydin, At\i{}l\i{}m G\"{u}nes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
title = {Automatic Differentiation in Machine Learning: A Survey},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "auto-diff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational uid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {5595-5637},
numpages = {43},
keywords = {differentiable programming, backpropagation}
}

@article{journals/corr/RonnebergerFB15,
  added-at = {2023-12-13T04:02:58.000+0100},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  biburl = {https://www.bibsonomy.org/bibtex/2b99e0743410b0939acaeb871134a21d7/admin},
  ee = {http://arxiv.org/abs/1505.04597},
  interhash = {9158de16b2caff7458df054dc6fc2748},
  intrahash = {b99e0743410b0939acaeb871134a21d7},
  journal = {CoRR},
  keywords = {},
  timestamp = {2023-12-13T04:02:58.000+0100},
  title = {U-Net: Convolutional Networks for Biomedical Image Segmentation.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1505.html#RonnebergerFB15},
  year = 2015
}

@inproceedings{10.5555/3045118.3045167,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
year = {2015},
publisher = {JMLR.org},
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {448-456},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}


@book {Asakura1989,
  author = {ì˙ñ{ó¨ëÃóÕäwâÔï“},
  title = {ó¨ëÃóÕäwÉVÉäÅ[ÉY},
  publisher = {í©ëqèëìX},
  year = {1989.7},
  URL = {https://ci.nii.ac.jp/ncid/BN03642451}
}

@book {Inamuro2020,
author = {àÓé∫ó≤ìÒ},
title = {äiéqÉ{ÉãÉcÉ}Éìñ@ì¸ñÂ : ï°éGã´äEÇ®ÇÊÇ—à⁄ìÆã´äEó¨ÇÍÇÃêîílåvéZñ@},
publisher = {ä€ëPèoî≈},
year = {2020.1},
URL = {https://ndlsearch.ndl.go.jp/books/R100000002-I03017878}
}